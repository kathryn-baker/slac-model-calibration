{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Predictions\n",
    "In this notebook we analyse the predictions made by the **uncalibrated** surrogate model with the machine data and evaluate the errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set it up to autoreload updates\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from utils import load_lcls, plot_series, chunk_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_pickle('data/full_2022-09-23_12_18_58-07_00__2022-09-23_23_03_01-07_00.pkl')\n",
    "full_data['beam_size'] = np.sqrt(full_data['OTRS:IN20:621:XRMS'] * full_data['OTRS:IN20:621:YRMS'])\n",
    "full_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "Because we're dealing with the raw PV data, we have to add an additional transformation layer to the model to convert the PV units to the simulation units. We do the same on the output layer as well, converting the scales from m to mm. We also have to use the PV names as the inputs rather than the simulation ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lume_model.utils import variables_from_yaml\n",
    "from botorch.models.transforms.input import AffineInputTransform, InputTransform\n",
    "from lume_model.torch import PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebuggingPyTorchModel(PyTorchModel):\n",
    "    def __init__(self, model_file: str, input_variables, output_variables, input_transformers, output_transformers, output_format, feature_order, output_order) -> None:\n",
    "        super().__init__(model_file, input_variables, output_variables, input_transformers, output_transformers, output_format, feature_order, output_order)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs/pv_info.json', 'r') as f:\n",
    "    pv_info = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "with open('configs/lcls_pv_variables.yml') as f:\n",
    "    input_variables, output_variables = variables_from_yaml(f)\n",
    "\n",
    "with open('configs/normalization.json', \"r\") as f:\n",
    "    norm_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(input_variables.keys())\n",
    "outputs = list(output_variables.keys())\n",
    "print(features)\n",
    "print(outputs)\n",
    "\n",
    "valid_keys = [key for key in features + outputs if key in full_data.columns] + ['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = full_data[valid_keys].dropna().reindex()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PVtoSimFactor(InputTransform, torch.nn.Module):\n",
    "    def __init__(self, conversion: torch.Tensor) -> None:\n",
    "        super().__init__()\n",
    "        self._conversion = conversion\n",
    "        self.transform_on_train = True\n",
    "        self.transform_on_eval = True\n",
    "        self.transform_on_fantasize = False\n",
    "\n",
    "    def transform(self, x):\n",
    "        return x * self._conversion\n",
    "\n",
    "    def untransform(self, x):\n",
    "        return x / self._conversion\n",
    "\n",
    "input_conversions = PVtoSimFactor(torch.tensor([pv_info['pv_to_sim_factor'][feature_name.replace('BCTRL', 'BDES')] for feature_name in features]))\n",
    "\n",
    "# converting from mm to m for measured sigma to sim sigma, leaving the others as is\n",
    "output_conversions = PVtoSimFactor(torch.tensor([pv_info['pv_to_sim_factor'][output] for output in outputs]))\n",
    "\n",
    "print(input_conversions._conversion)\n",
    "print(output_conversions._conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = []\n",
    "for ele in [\"x\", \"y\"]:\n",
    "    scale = torch.tensor(norm_data[f\"{ele}_scale\"], dtype=torch.double)\n",
    "    min_val = torch.tensor(norm_data[f\"{ele}_min\"], dtype=torch.double)\n",
    "    transform = AffineInputTransform(\n",
    "        len(norm_data[f\"{ele}_min\"]),\n",
    "        1 / scale,\n",
    "        -min_val / scale,\n",
    "    )\n",
    "\n",
    "    transformers.append(transform)\n",
    "\n",
    "nn_model = PyTorchModel(\n",
    "    'torch_model.pt',\n",
    "    input_variables,\n",
    "    output_variables,\n",
    "    input_transformers=[input_conversions, transformers[0]],\n",
    "    output_transformers=[transformers[1], output_conversions], #,  # first we go from nn to sim units, then from sim_units to PV units\n",
    "    feature_order=features,\n",
    "    output_order=outputs\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "Use the measured data to make predictions for each of the output values. \n",
    "\n",
    "**NOTE** for now we ignore the values of input where the measured data is outside of the training range as this causes the errors to become enormous. This might be a useful indicator in future for when to retrain / gather new simulations to retrain the base model. These cannot be retrained using a calibration layer because the errors make the training too unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_features = ['IRIS:LR20:130:CONFG_SEL','ACCL:IN20:400:L0B_ADES']  # we ignore these because all of the values are outside the training range\n",
    "# ignored_features = []\n",
    "\n",
    "input_dict = {}\n",
    "for feature in features:\n",
    "    if feature not in ignored_features:\n",
    "        try:\n",
    "            input_dict[feature] = torch.from_numpy(data[feature].values)\n",
    "        except KeyError:\n",
    "            # if we get a key error then we don't know what the name of the PV for this is, so we use the default.\n",
    "            print(feature)\n",
    "    else:\n",
    "        print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['timestamp']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't ignore these features outside of the training range, we get unphysical values of sigma_x and sigma_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[list(input_dict.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nn_model.evaluate(input_dict)\n",
    "result['beam_size'] = torch.sqrt(result['OTRS:IN20:621:XRMS'] * result['OTRS:IN20:621:YRMS'])\n",
    "\n",
    "result = {key: value.detach().numpy() for key, value in result.items()}\n",
    "\n",
    "model_df = pd.DataFrame(result)\n",
    "print(len(model_df))\n",
    "model_df['timestamp'] = data['timestamp'].copy()\n",
    "print(len(model_df))\n",
    "# TODO fix this - work out why it's making it different lengths\n",
    "# model_df = pd.concat([data[list(input_dict.keys())].copy(), model_df], axis=1)\n",
    "print(len(model_df))\n",
    "model_df[outputs[0:2]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_df[list(input_dict.keys())].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[outputs[0:2]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_series(data, columns=list(input_dict.keys())+['OTRS:IN20:621:XRMS', 'OTRS:IN20:621:YRMS'], pred_df=model_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[outputs[0:2]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df[outputs[0:2]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_dfs = chunk_dataset(data)\n",
    "chunked_model_dfs = chunk_dataset(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for true_df, pred_df in zip(chunked_dfs, chunked_model_dfs):\n",
    "    fig, ax = plot_series(true_df, columns=list(input_dict.keys())+['OTRS:IN20:621:XRMS', 'OTRS:IN20:621:YRMS'], pred_df=pred_df)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lume-epics-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
