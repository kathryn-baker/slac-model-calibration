{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Calibration Layer\n",
    "In this notebook we analyse whether an additional calibration layer can be used to improve the performance of model on a dataset where some offset has been applied to simulate a miscalibration in a sensor. We do the study in **PV units**, adding the calibration layer BEFORE the PV -> sim transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_lcls\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from lume_model.torch import LUMEModule\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from torch.nn import MSELoss\n",
    "from botorch.models.transforms.input import InputTransform, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs/pv_info.json', 'r') as f:\n",
    "    pv_info = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "pv_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = load_lcls('configs/lcls_variables.yml', 'configs/normalization.json')\n",
    "output_transformer = deepcopy(nn_model.output_transformers[0])\n",
    "input_transformer = deepcopy(nn_model._input_transformers[0])\n",
    "# we remove the output transformation so we can make comparisons between the outcomes using MSE\n",
    "nn_model._output_transformers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = [feature_name for feature_name, var in nn_model.input_variables.items() if var.value_range[0] == var.value_range[1]]\n",
    "print(constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.from_numpy(np.load('data/x_raw_small.npy', allow_pickle=True).astype('float64'))\n",
    "y_test = torch.from_numpy(np.load('data/y_raw_small.npy', allow_pickle=True).astype('float64'))\n",
    "y_test = output_transformer(y_test)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversions = torch.tensor([pv_info['pv_to_sim_factor'][pv_info['sim_name_to_pv_name'][feature_name]] for feature_name in nn_model.features])\n",
    "\n",
    "class PVtoSimFactor(InputTransform, torch.nn.Module):\n",
    "    def __init__(self, conversion: torch.Tensor) -> None:\n",
    "        super().__init__()\n",
    "        self._conversion = conversion\n",
    "        self.transform_on_train = True\n",
    "        self.transform_on_eval = True\n",
    "        self.transform_on_fantasize = False\n",
    "\n",
    "    def transform(self, x):\n",
    "        return x * self._conversion\n",
    "\n",
    "    def untransform(self, x):\n",
    "        return x / self._conversion\n",
    "    \n",
    "pv_to_sim = PVtoSimFactor(conversions)\n",
    "x_test_pv = pv_to_sim.untransform(x_test)\n",
    "x_test_transformed = pv_to_sim.transform(x_test_pv)\n",
    "\n",
    "# verify that the transformations work as expected\n",
    "print(x_test)\n",
    "print(x_test_pv)\n",
    "print(x_test_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add the transformation from PV to sim into the input transformations of the model\n",
    "# and use it to determine the error on the true dataset and the miscalibrated dataset\n",
    "nn_model._input_transformers.insert(0, pv_to_sim)\n",
    "print(nn_model.input_transformers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to apply a given offset to each of the inputs, which we assume as some percentage of the mean measurement value and transform the original dataset using these offsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiscalibratedInput():\n",
    "    def __init__(self, x, scales=None, offset_percentage=None) -> None:\n",
    "        self.offsets = self.calculate_offsets(x, offset_percentage)\n",
    "        self.scales = self.calculate_scales(x, scales)\n",
    "\n",
    "    def calculate_offsets(self, x, offset_percentage):\n",
    "        if offset_percentage is None:\n",
    "            # if no value is passed, we assume a 1% offset\n",
    "            offset_percentages = torch.full((x.shape[-1],),0.1)\n",
    "        elif isinstance(offset_percentage,float):\n",
    "            offset_percentages = torch.full((x.shape[-1],),offset_percentage)\n",
    "        else:\n",
    "            if offset_percentage.shape[-1] != x.shape[-1]:\n",
    "                raise ValueError(\n",
    "                    f\"\"\"length of passed scales should equal the number of features in the dataset.\\\n",
    "                         Recevied: {offset_percentage.shape[-1]}, Expected: {x.shape[-1]}\n",
    "                    \"\"\"\n",
    "                )\n",
    "            offset_percentages = offset_percentage\n",
    "        \n",
    "        # using the percentages, we apply an offset of some percentage of the mean value\n",
    "        mean_vals = []\n",
    "\n",
    "        for i in range(x.shape[-1]):\n",
    "            if x[...,i].std() == 0:\n",
    "                mean = 1e-6\n",
    "            else:\n",
    "                mean = x_test_pv[:,i].mean().item()\n",
    "            mean_vals.append(mean)\n",
    "\n",
    "        mean_vals = torch.tensor(mean_vals)\n",
    "\n",
    "        offsets = mean_vals * offset_percentages\n",
    "        return offsets\n",
    "\n",
    "    def calculate_scales(self, x, scale):\n",
    "        if scale is None:\n",
    "            scales = torch.ones(x.shape[-1])\n",
    "        elif isinstance(scale, float):\n",
    "            scales = torch.full((x.shape[-1]), scale)\n",
    "        else:\n",
    "            if scale.shape[-1] != x.shape[-1]:\n",
    "                raise ValueError(\n",
    "                    f\"\"\"length of passed scales should equal the number of features in the dataset.\\\n",
    "                         Recevied: {scale.shape[-1]}, Expected: {x.shape[-1]}\n",
    "                    \"\"\"\n",
    "                )\n",
    "            scales = scale\n",
    "        return scales\n",
    "    \n",
    "    def get_data(self, x):\n",
    "        return x * self.scales + self.offsets\n",
    "    \n",
    "mis_cal_input = MiscalibratedInput(\n",
    "    x_test_pv,\n",
    "    scales=torch.tensor([1.0 if feature_name not in constants  else 1.0 for feature_name in nn_model.features ]),\n",
    "    offset_percentage=torch.full((len(nn_model.features),), 0.1)\n",
    ")\n",
    "x_test_pv_offset = mis_cal_input.get_data(x_test_pv)\n",
    "print(x_test_pv)\n",
    "print(x_test_pv_offset)\n",
    "print(mis_cal_input.offsets)\n",
    "print(mis_cal_input.scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.input_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LUMEModule(nn_model, nn_model.features, nn_model.outputs)\n",
    "\n",
    "no_offset_results = torch.transpose(base_model(x_test_pv).detach(), 1,0)\n",
    "offset_results = torch.transpose(base_model(x_test_pv_offset).detach(), 1,0)\n",
    "\n",
    "sort_idx = torch.argsort(y_test[:,0])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_test[:,0][sort_idx], '.', label='y true')\n",
    "ax.plot(no_offset_results[:,0][sort_idx], '.', label='y no offset')\n",
    "ax.plot(offset_results[:,0][sort_idx], '.', label='y offset')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large error in the miscalibrated model is likely due to the constant values being offset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this offset dataset, we want to add an additional calibration layer prior to the PV->SIM conversion that will allow us to determine what value will bring the offset dataset back to the known domain. \n",
    "\n",
    "In order to train this calibration layer, the weights of the offset need to be normalised to a similar range for each feature to prevent some features from dominating over others during training, which helps to stabilise the training process. \n",
    "\n",
    "We train this normalisation layer using the known conversion dataset that we originally have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_normalization = Normalize(len(nn_model.features))\n",
    "pv_normalization.train()\n",
    "pv_normalization(x_test_pv)\n",
    "pv_normalization.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalibrationLayer(torch.nn.Module):\n",
    "    def __init__(self, dim, input_transformer) -> None:\n",
    "        super().__init__()\n",
    "        self._input_transformer = input_transformer\n",
    "        self.offsets = torch.nn.Parameter(torch.full((dim,),1e-6), requires_grad=True)\n",
    "        # for now we assume that the scales are set, we only have offsets\n",
    "        self.scales = torch.nn.Parameter(torch.ones(dim) , requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self._input_transformer.eval()\n",
    "        x = self._input_transformer(x)\n",
    "        x = x * self.scales + self.offsets\n",
    "        x = self._input_transformer.untransform(x)\n",
    "        return x\n",
    "\n",
    "cal_layer = CalibrationLayer(len(nn_model.features), pv_normalization)\n",
    "# check it can be called, for now it should return the exact same values\n",
    "print(cal_layer(x_test_pv_offset))\n",
    "print(x_test_pv_offset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add this layer into the input transformations for our LUMEModel and run it through a training loop using the offset dictionary and the true output values. \n",
    "\n",
    "NOTE what do we use as y here? The x_test_pv values or the y_true values. My gut tells me to use the x_test_pv values as we want the calibration layer to learn a transformation to bring the value back to what the original model was predicting, not some unknown uncertainty in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = deepcopy(x_test_pv_offset)\n",
    "y_train = torch.transpose(deepcopy(base_model(x_test_pv).detach()),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LUMEModuleT(LUMEModule):\n",
    "    def __init__(self, model, feature_order, output_order) -> None:\n",
    "        super().__init__(model, feature_order, output_order)\n",
    "    def forward(self, x):\n",
    "        result = super().forward(x)\n",
    "        return torch.transpose(result, 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_layer = CalibrationLayer(len(nn_model.features), pv_normalization)\n",
    "calibrated_nn = deepcopy(nn_model)\n",
    "calibrated_nn._input_transformers.insert(0,cal_layer)\n",
    "\n",
    "calibrated_model = LUMEModuleT(calibrated_nn, calibrated_nn.features, calibrated_nn.outputs)\n",
    "calibrated_model.register_parameter('offsets', cal_layer.offsets)\n",
    "# calibrated_model.register_parameter('scales', cal_layer.scales)\n",
    "\n",
    "# now we define a training loop that trains the offsets\n",
    "loss_fn = torch.nn.MSELoss()  # mean square error\n",
    "optimizer = torch.optim.Adam(calibrated_model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5, verbose=False)\n",
    "\n",
    "\n",
    "n_epochs = 500   # number of epochs to run\n",
    "\n",
    "# Hold the best model\n",
    "best_mse = torch.inf   # init to infinity\n",
    "best_weights = None\n",
    "val_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    calibrated_model.train()\n",
    "    # forward pass\n",
    "    y_pred = calibrated_model(x)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    # evaluate accuracy at end of each epoch\n",
    "    calibrated_model.eval()\n",
    "    y_pred = calibrated_model(x)\n",
    "    mse = loss_fn(y_pred, y_test).item()\n",
    "    val_history.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_weights = deepcopy(calibrated_model.state_dict())\n",
    " \n",
    "# restore calibrated_model and return best accuracy\n",
    "calibrated_model.load_state_dict(best_weights)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(val_history[int(0.1*n_epochs):])\n",
    "# ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is that as soon as any offset is introduced into the constant values, the error skyrockets and training completely breaks down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal_layer.offsets)\n",
    "print(cal_layer.scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_layer.eval()\n",
    "learned_real_offsets = x_test_pv_offset - (cal_layer(x_test_pv_offset) / cal_layer.scales).detach()\n",
    "print(learned_real_offsets[0])\n",
    "print(mis_cal_input.offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,sharex='all',figsize=(15,10))\n",
    "\n",
    "ax[0].bar(range(16), mis_cal_input.offsets, label='true offset',alpha=0.5)\n",
    "ax[0].bar(range(16), learned_real_offsets[0], label='learned offset',alpha=0.5)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].bar(range(16), mis_cal_input.scales, label='true offset',alpha=0.5)\n",
    "ax[1].bar(range(16), cal_layer.scales.detach(), label='learned offset',alpha=0.5)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.xticks(range(16), nn_model.features,rotation=90)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compare what the calibrated results look like versus the original model results\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "no_offset_results = torch.transpose(base_model(x_test_pv).detach(), 1,0)\n",
    "offset_results_no_calibration = torch.transpose(base_model(x_test_pv_offset).detach(), 1,0)\n",
    "offset_results_with_calibration = calibrated_model(x_test_pv_offset).detach()\n",
    "\n",
    "ax.plot(no_offset_results[:,0][sort_idx], '.', label='base no offset')\n",
    "ax.plot(offset_results_no_calibration[:,0][sort_idx], '.', label='offset no calibration')\n",
    "ax.plot(offset_results_with_calibration[:,0][sort_idx], '.', label='offset with calibration')\n",
    "ax.plot(y_test[:,0][sort_idx], 'k--', label='GT')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lume-epics-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
