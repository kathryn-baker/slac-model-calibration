{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "In order to use this data for training a model for calibrtion offset, we want to make sure that the distributions of the data are correct and don't include any outliers that might skew the data. \n",
    "\n",
    "Similarly, there are some features that we don't want to use because the distributions seen in the raw machine data differ too greatly from those seen during training (e.g. trained with a constant value but distributions seen in reality, ranges outside of the training range).\n",
    "\n",
    "There are also some features in the model that are combinations of multiple input PVs so we need to create those features and mappings ourselves. \n",
    "\n",
    "The goal of this notebook is to condense the dataframe of all of the PVs recorded down to a dataframe that can be used for training calibration layers.\n",
    "\n",
    "To Do:\n",
    "1. [x] subset data to include only relevant features\n",
    "1. [x] remove unphysical values\n",
    "1. [x] create compound features\n",
    "1. [x] remove outliers (anything more or less than 3*std)\n",
    "1. [x] drop any points where not all features are present\n",
    "    1. *save here* incase we want to use the normal features as well\n",
    "1. [?] filter points where beam size changes without features changing\n",
    "1. [x] replace ignored features with default values (save as ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from plotting import plot_series, plot_boxplot, plot_histogram\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from adtk.transformer import RegressionResidual\n",
    "from utils import get_date_from_df\n",
    "from datetime import datetime\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('archive_data/raw/injector_2021-11-16.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series = pd.read_pickle('archive_data\\injector_2021-04-14.pkl')\n",
    "# time_series = time_series['2022-09-23 12:00:00':'2022-09-23 23:59:59']\n",
    "original_time_series = [pd.read_pickle(filename) for filename in glob.glob('archive_data/injector_*.pkl')]\n",
    "all_time_series = [pd.read_pickle(filename) for filename in glob.glob('archive_data/injector_*.pkl')]\n",
    "print(len(all_time_series))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(all_time_series[0], show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_series[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_series[0].columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframes will all contain both the actual and the control values for the quadrupole settings. For now the intention is to do all of the data **processing** using the CONTROL PVs (e.g. dropping duplicates etc) and then using the ACTUAL PVs for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for time_series in all_time_series:\n",
    "#     # date = str(time_series.index[0].to_pydatetime().date())\n",
    "#     fig, ax = plot_series(time_series)\n",
    "#     fig.savefig(f'archive_data/data_exploration/time_series/{get_date_from_df(time_series)}.png')\n",
    "#     plt.close(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant PVs\n",
    "Here we take the subset of PVs of interest from the full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_features = [\n",
    "    # 'IRIS:LR20:130:CONFG_SEL',  # out of range - swapping to use sigma_x and sigma_y instead\n",
    "    'ACCL:IN20:400:L0B_ADES',  # out of range - trained as constant but varying at value outside of this\n",
    "    'Pulse_length',  # not measured as PV - assume is fixed at reference\n",
    "    'FBCK:BCI0:1:CHRG_S'  # trained as constant but measured a distribution\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs/pv_info.json', 'r') as f:\n",
    "    pv_info = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "with open('configs/model_info.json', 'r') as f:\n",
    "    model_info = json.load(f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_series[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dictionary contains the mappings for the outputs as well as the inputs so we don't need to specify the outputs separately\n",
    "relevant_pvs = []\n",
    "for pv_name in list(pv_info['pv_name_to_sim_name'].keys()) + ['CAMR:IN20:186:YRMS', 'CAMR:IN20:186:XRMS']:\n",
    "    if pv_name in all_time_series[0].columns:\n",
    "        relevant_pvs.append(pv_name.replace('BCTRL', 'BACT'))\n",
    "    # if 'BACT' in pv_name and pv_name.replace('BCTRL','BACT') in all_time_series[0].columns:\n",
    "    #     # we want to include the actual and the control points just in case\n",
    "    #     relevant_pvs.append(pv_name.replace('BCTRL','BACT'))\n",
    "\n",
    "print(relevant_pvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_na_and_duplicates(time_series, relevant_pvs):\n",
    "    # drop any NA rows\n",
    "    time_series = time_series[relevant_pvs].dropna()\n",
    "    # drop any duplicated columns\n",
    "    time_series = time_series.loc[:,~time_series.columns.duplicated()]\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_series = [drop_na_and_duplicates(time_series, relevant_pvs) for time_series in all_time_series]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rdist(time_series):\n",
    "    r_dist = np.sqrt(time_series['CAMR:IN20:186:XRMS'].values**2 + time_series['CAMR:IN20:186:YRMS'].values**2)\n",
    "    time_series['CAMR:IN20:186:R_DIST'] = r_dist\n",
    "\n",
    "    # once we have it we can drop these other two dolumns\n",
    "    time_series = time_series.drop('CAMR:IN20:186:XRMS',axis=1)\n",
    "    time_series = time_series.drop('CAMR:IN20:186:YRMS', axis=1)\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_series = [add_rdist(time_series) for time_series in all_time_series]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Unphysical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measured_pvs = [pv for pv in relevant_pvs if pv not in ignored_features]\n",
    "# print(measured_pvs)\n",
    "# print(len(measured_pvs))\n",
    "measured_pvs = [pvname for pvname in pv_info['pv_name_to_sim_name'].keys() if pvname in all_time_series[0].columns and pvname not in ignored_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define the ranges of the PVs that we're interested in\n",
    "thresholds = {}\n",
    "for pv in pv_info['pv_name_to_sim_name']:\n",
    "    sim_name = pv_info['pv_name_to_sim_name'][pv]\n",
    "    sim_to_pv_factor = pv_info['sim_to_pv_factor'][sim_name]\n",
    "    if sim_name in model_info['loc_in']:\n",
    "        sim_idx = model_info['loc_in'][sim_name]\n",
    "        lower_bound = model_info['train_input_mins'][sim_idx] * sim_to_pv_factor\n",
    "        upper_bound = model_info['train_input_maxs'][sim_idx] * sim_to_pv_factor\n",
    "        if lower_bound != upper_bound:\n",
    "            if lower_bound < upper_bound:\n",
    "                thresholds[pv] = {\n",
    "                    'lower': lower_bound,\n",
    "                    'upper': upper_bound,\n",
    "                }\n",
    "            else:\n",
    "                thresholds[pv] = {\n",
    "                    'lower': upper_bound,\n",
    "                    'upper': lower_bound,\n",
    "                }\n",
    "\n",
    "\n",
    "thresholds.update(\n",
    "    {\n",
    "        'OTRS:IN20:571:XRMS': {'lower': 0, 'upper': 1000},\n",
    "        'OTRS:IN20:571:YRMS': {'lower': 0, 'upper': 1000},\n",
    "        # 'CAMR:IN20:186:R_DIST': {'lower': 250, }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unphysical_and_ood_values(time_series, measured_pvs, thresholds):\n",
    "    original_len = len(time_series)\n",
    "    for pv in measured_pvs:\n",
    "        if pv in thresholds.keys():\n",
    "            time_series = time_series[(time_series[pv] > thresholds[pv].get('lower')) & (time_series[pv] < thresholds[pv].get('upper'))]\n",
    "\n",
    "    final_len = len(time_series)\n",
    "    print(f'Removed {original_len - final_len} points from time series of {original_len} points')\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_series = [df for df in [remove_unphysical_and_ood_values(time_series, measured_pvs, thresholds) for time_series in all_time_series] if len(df > 0)]\n",
    "# print(len(test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers\n",
    "Here we inspect each of the histograms and determine what outlier threshold we should be using for each input feature (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(measured_pvs))\n",
    "# print(measured_pvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_pvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_pvs = [pvname for pvname in pv_info['pv_name_to_sim_name'].keys() if pvname in all_time_series[0].columns and pvname not in ignored_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_pvs\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(measured_pvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_series in all_time_series:\n",
    "    plot_boxplot(time_series, measured_pvs, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df,columns,n_std):\n",
    "    original_len = len(df)\n",
    "    df = df.copy()\n",
    "    for col in columns:       \n",
    "        mean = df[col].mean()\n",
    "        sd = df[col].std()\n",
    "        \n",
    "        df = df[(df[col] <= mean+(n_std*sd))]\n",
    "    final_len = len(df)\n",
    "    print(f'Removed {original_len - final_len} points from time series of {original_len} points')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_series =[df for df in [remove_outliers(time_series, measured_pvs, 3) for time_series in all_time_series] if len(df > 0)]\n",
    "# time_series = remove_outliers(time_series, measured_pvs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_series in all_time_series:\n",
    "    plot_histogram(time_series, measured_pvs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwrite Ignored Values\n",
    "For some values where the range is far outside that of the training data, we want to use the reference point value instead of the measured value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.ref_config import ref_point\n",
    "ref_point = ref_point[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwrite_ignored_features(time_series, model_info, pv_info, ref_point, ignored_features):\n",
    "    for pvname in ignored_features:\n",
    "        sim_name = pv_info['pv_name_to_sim_name'][pvname]\n",
    "        # ref points are in sim values so we need to convert to pv_units\n",
    "        feature_loc = model_info['loc_in'][sim_name]\n",
    "        reference_val = pv_info['sim_to_pv_factor'][sim_name] * ref_point[feature_loc]\n",
    "        time_series[pvname] = reference_val\n",
    "        print(f'reset {pvname} to {reference_val}')\n",
    "    return time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_series = [overwrite_ignored_features(time_series, model_info, pv_info, ref_point, ignored_features) for time_series in all_time_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series_subset = time_series_subset.copy()\n",
    "ignored_outputs = ['sigma_z', 'norm_emit_x', 'norm_emit_y']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop NAs\n",
    "Here we drop the items in the dataframe where not all features are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_series = [time_series.dropna() for time_series in all_time_series]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stray Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [pv_info['sim_name_to_pv_name'][sim_name] for sim_name in model_info['model_in_list'] if pv_info['sim_name_to_pv_name'][sim_name] in time_series.columns]\n",
    "# varying_features = [pvname for pvname in features if 'SOL' in pvname or 'QUAD' in pvname]\n",
    "# outputs = ['OTRS:IN20:571:XRMS' ,'OTRS:IN20:571:YRMS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_series in all_time_series:\n",
    "    plot_series(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_series(time_series,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_vals = pd.Series({\n",
    "#     'CAMR:IN20:186:R_DIST': 453.46957443120476,\n",
    "#     'Pulse_length': 1.8550514181818183,\n",
    "#     'FBCK:BCI0:1:CHRG_S': 0.25,\n",
    "#     'SOLN:IN20:121:BACT': 0.47600114995527293,\n",
    "#     'QUAD:IN20:121:BACT': 0.014395751763488018,\n",
    "#     'QUAD:IN20:122:BACT': 0.011783769915062949,\n",
    "#     'ACCL:IN20:300:L0A_ADES': 58.0,\n",
    "#     'ACCL:IN20:300:L0A_PDES': 0.0,\n",
    "#     'ACCL:IN20:400:L0B_ADES': 70.0,\n",
    "#     'ACCL:IN20:400:L0B_PDES': -2.5,\n",
    "#     'QUAD:IN20:361:BACT': -3.3887000630248605,\n",
    "#     'QUAD:IN20:371:BACT': 2.721178164561805,\n",
    "#     'QUAD:IN20:425:BACT': -2.173085321374461,\n",
    "#     'QUAD:IN20:441:BACT': -0.014265294503135854,\n",
    "#     'QUAD:IN20:511:BACT': 2.8476903766544197,\n",
    "#     'QUAD:IN20:525:BACT': -2.7030069765876372,\n",
    "#     'OTRS:IN20:621:XRMS': 88.19116289583675,\n",
    "#     'OTRS:IN20:621:YRMS': 73.2518868482159\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_series = pd.read_pickle('data\\calibration_2022-09-23_12_44_44.808775424-07_00__2022-09-23_23_59_58.905014016-07_00.pkl')\n",
    "# time_series = time_series[0:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for time_series in all_time_series:\n",
    "#     print(len(time_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_stray_periods(time_series, features, outputs):\n",
    "#     sx = RegressionResidual(regressor=LinearRegression(), target=outputs[0]).fit_transform(time_series[features+outputs])\n",
    "#     sy = RegressionResidual(regressor=LinearRegression(), target=outputs[1]).fit_transform(time_series[features+outputs])\n",
    "\n",
    "#     sx = sx[abs(sx) < 20]\n",
    "#     sy = sy[abs(sy) < 20]\n",
    "\n",
    "#     combo_points = set(sx.index).intersection(set(sy.index))\n",
    "#     test = time_series.loc[list(combo_points)].sort_index()\n",
    "#     return test\n",
    "\n",
    "# # test = remove_stray_periods(time_series, varying_features, outputs)\n",
    "# # print(len(time_series))\n",
    "# # print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_time_series = [df for df in [remove_stray_periods(time_series, varying_features, outputs) for time_series in all_time_series] if len(df) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for time_series in all_time_series:\n",
    "#     print(len(time_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_series in all_time_series:\n",
    "    plot_series(time_series, save_dir='time_series_filtered')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorder Features\n",
    "Here we reorder the features into the order that the neural network model expects and set the timestamp as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_features(time_series, features, outputs):\n",
    "    time_series = time_series[features+outputs]\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  [pv_info['sim_name_to_pv_name'][sim_name] for sim_name in model_info['model_in_list'] if pv_info['sim_name_to_pv_name'][sim_name] in time_series.columns]\n",
    "outputs =  [pv_info['sim_name_to_pv_name'][sim_name] for sim_name in model_info['model_out_list'] if pv_info['sim_name_to_pv_name'][sim_name] in time_series.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time_series = [reorder_features(time_series, features, outputs) for time_series in all_time_series]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "Once all the processing is complete, we save the dataset with only the features and outputs of iterest to us during training. We also split the data into training, validation and test sets based on the month. \n",
    "\n",
    "For now we will use all of the data from December as validation / test data and the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "validation_data = []\n",
    "test_data = []\n",
    "    \n",
    "for time_series in all_time_series:\n",
    "    date_str = get_date_from_df(time_series)\n",
    "    date = datetime(*(int(i) for i in date_str.split('-')))\n",
    "    if date.month == 12:\n",
    "        if date.day == 9:\n",
    "            test_data.append(time_series.drop_duplicates())\n",
    "    elif date.day == 18 and date.month == 11:\n",
    "        validation_data.append(time_series[\"2021-11-18 00:00:00-0800\": \"2021-11-18 09:00:00-0800\"].drop_duplicates())\n",
    "        training_data.append(time_series[\"2021-11-18 09:00:01-0800\": \"2021-11-19 00:00:00-0800\"].drop_duplicates())\n",
    "    else:\n",
    "        training_data.append(time_series.drop_duplicates())\n",
    "\n",
    "train_df = pd.concat(training_data).sort_index()\n",
    "val_df = pd.concat(validation_data).sort_index()\n",
    "test_df = pd.concat(test_data).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check there's no overlap between the datasets\n",
    "\n",
    "train_set = set(train_df.index)\n",
    "val_set = set(val_df.index)\n",
    "test_set = set(test_df.index)\n",
    "\n",
    "print(len(train_set.intersection(val_set)) == 0)\n",
    "print(len(val_set.intersection(test_set)) == 0)\n",
    "print(len(train_set.intersection(test_set)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle('archive_data/train_df.pkl')\n",
    "val_df.to_pickle('archive_data/val_df.pkl')\n",
    "test_df.to_pickle('archive_data/test_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
