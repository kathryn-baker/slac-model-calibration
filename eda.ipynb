{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data \n",
    "Using captum following [this tutorial](https://captum.ai/tutorials/House_Prices_Regression_Interpret) for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = json.load(open(\"configs/model_info.json\"))\n",
    "pv_info = json.load(open(\"configs/pv_info.json\"))\n",
    "nn_transform_info = json.load(open(\"configs/normalization.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_sim_to_nn_transformers\n",
    "from transformed_model import KeyedTransformedModel\n",
    "# get transformers for normailzation into NN\n",
    "nn_input_transformer, nn_output_transformer = create_sim_to_nn_transformers(\n",
    "    \"configs/normalization.json\"\n",
    ")\n",
    "test_min = torch.tensor(model_info[\"train_input_mins\"])\n",
    "test_max = torch.tensor(model_info[\"train_input_maxs\"]).unsqueeze(0)\n",
    "\n",
    "model = torch.load(\"torch_model.pt\").double()\n",
    "\n",
    "# define the NN surrogate that contains the NN, the input/output transformers for\n",
    "# simulation units\n",
    "surrogate = KeyedTransformedModel(\n",
    "    model,\n",
    "    nn_input_transformer,\n",
    "    nn_output_transformer,\n",
    "    model_info[\"model_in_list\"],\n",
    "    model_info[\"model_out_list\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_x_data = np.load(\"data/x_raw_small.npy\", allow_pickle=True)\n",
    "raw_y_data = np.load(\"data/y_raw_small.npy\", allow_pickle=True).astype('float')\n",
    "\n",
    "x_df = pd.DataFrame(raw_x_data, columns=model_info['model_in_list'])\n",
    "y_df = pd.DataFrame(raw_y_data, columns=model_info['model_out_list'])\n",
    "\n",
    "preds = surrogate(torch.tensor(raw_x_data).double())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick one of the output variables to see how it varies with each input. Once we have the process nailed down we can repeat this for the other four outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_var = 'sigma_x'\n",
    "feature_names = model_info['model_in_list']\n",
    "print(model_info['model_out_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows = 4, ncols=4, sharey=True, figsize=(20,15))\n",
    "for i, (ax, col) in enumerate(zip(axs.flat, feature_names)):    \n",
    "    x = raw_x_data[:,i]\n",
    "    pf = np.polyfit(x, raw_y_data[:,0], 1)\n",
    "    p = np.poly1d(pf)\n",
    "\n",
    "    ax.plot(x, raw_y_data[:,0], 'o')\n",
    "    ax.plot(x, p(x),\"r--\")\n",
    "\n",
    "    ax.set_title(col + ' vs sigma_x')\n",
    "    ax.set_xlabel(col)\n",
    "# axs[:,0].set_ylabel('sigma_x')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, output_name in enumerate(model_info['model_out_list']):\n",
    "    ig = IntegratedGradients(model)\n",
    "    ig_nt = NoiseTunnel(ig)\n",
    "    dl = DeepLift(model)\n",
    "    gs = GradientShap(model)\n",
    "    fa = FeatureAblation(model)\n",
    "\n",
    "    X_test = torch.Tensor(raw_x_data).double()\n",
    "\n",
    "    # NOTE for multi-output problems you need to pass the target value of the output you're interested\n",
    "    # in, for example here we are interested in the first prediction (sigma_x) so we pass target=0\n",
    "\n",
    "    ig_attr_test = ig.attribute(X_test, target=idx, n_steps=50)\n",
    "    ig_nt_attr_test = ig_nt.attribute(X_test, target=idx)\n",
    "    dl_attr_test = dl.attribute(X_test, target=idx)\n",
    "    fa_attr_test = fa.attribute(X_test, target=idx)\n",
    "\n",
    "    # plot feature importances\n",
    "    x_axis_data = np.arange(X_test.shape[1])\n",
    "    x_axis_data_labels = list(map(lambda idx: feature_names[idx], x_axis_data))\n",
    "\n",
    "    ig_attr_test_sum = ig_attr_test.detach().numpy().sum(0)\n",
    "    ig_attr_test_norm_sum = ig_attr_test_sum / np.linalg.norm(ig_attr_test_sum, ord=1)\n",
    "\n",
    "    ig_nt_attr_test_sum = ig_nt_attr_test.detach().numpy().sum(0)\n",
    "    ig_nt_attr_test_norm_sum = ig_nt_attr_test_sum / np.linalg.norm(ig_nt_attr_test_sum, ord=1)\n",
    "\n",
    "    dl_attr_test_sum = dl_attr_test.detach().numpy().sum(0)\n",
    "    dl_attr_test_norm_sum = dl_attr_test_sum / np.linalg.norm(dl_attr_test_sum, ord=1)\n",
    "\n",
    "    fa_attr_test_sum = fa_attr_test.detach().numpy().sum(0)\n",
    "    fa_attr_test_norm_sum = fa_attr_test_sum / np.linalg.norm(fa_attr_test_sum, ord=1)\n",
    "\n",
    "    width = 0.14\n",
    "    legends = ['Int Grads', 'Int Grads w/SmoothGrad','DeepLift', 'Feature Ablation']\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    ax = plt.subplot()\n",
    "    ax.set_title('Comparing input feature importances across multiple algorithms and learned weights')\n",
    "    ax.set_ylabel('Attributions')\n",
    "\n",
    "    ax.bar(x_axis_data, ig_attr_test_norm_sum, width, align='center', alpha=0.8, color='#eb5e7c')\n",
    "    ax.bar(x_axis_data + width, ig_nt_attr_test_norm_sum, width, align='center', alpha=0.7, color='#A90000')\n",
    "    ax.bar(x_axis_data + 2 * width, dl_attr_test_norm_sum, width, align='center', alpha=0.6, color='#34b8e0')\n",
    "    ax.bar(x_axis_data + 3 * width, fa_attr_test_norm_sum, width, align='center', alpha=1.0, color='#49ba81')\n",
    "    ax.autoscale_view()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    ax.set_xticks(x_axis_data + 0.5)\n",
    "    ax.set_xticklabels(x_axis_data_labels)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend(legends, loc=3)\n",
    "    plt.title(output_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "ig_nt = NoiseTunnel(ig)\n",
    "dl = DeepLift(model)\n",
    "gs = GradientShap(model)\n",
    "fa = FeatureAblation(model)\n",
    "\n",
    "X_test = torch.Tensor(raw_x_data).double()\n",
    "\n",
    "# NOTE for multi-output problems you need to pass the target value of the output you're interested\n",
    "# in, for example here we are interested in the first prediction (sigma_x) so we pass target=0\n",
    "\n",
    "ig_attr_test = ig.attribute(X_test, target=0, n_steps=50)\n",
    "ig_nt_attr_test = ig_nt.attribute(X_test, target=0)\n",
    "dl_attr_test = dl.attribute(X_test, target=0)\n",
    "# gs_attr_test = gs.attribute(X_test, target=0, X_train)\n",
    "fa_attr_test = fa.attribute(X_test, target=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis_data = np.arange(X_test.shape[1])\n",
    "x_axis_data_labels = list(map(lambda idx: feature_names[idx], x_axis_data))\n",
    "\n",
    "ig_attr_test_sum = ig_attr_test.detach().numpy().sum(0)\n",
    "ig_attr_test_norm_sum = ig_attr_test_sum / np.linalg.norm(ig_attr_test_sum, ord=1)\n",
    "\n",
    "ig_nt_attr_test_sum = ig_nt_attr_test.detach().numpy().sum(0)\n",
    "ig_nt_attr_test_norm_sum = ig_nt_attr_test_sum / np.linalg.norm(ig_nt_attr_test_sum, ord=1)\n",
    "\n",
    "dl_attr_test_sum = dl_attr_test.detach().numpy().sum(0)\n",
    "dl_attr_test_norm_sum = dl_attr_test_sum / np.linalg.norm(dl_attr_test_sum, ord=1)\n",
    "\n",
    "fa_attr_test_sum = fa_attr_test.detach().numpy().sum(0)\n",
    "fa_attr_test_norm_sum = fa_attr_test_sum / np.linalg.norm(fa_attr_test_sum, ord=1)\n",
    "\n",
    "width = 0.14\n",
    "legends = ['Int Grads', 'Int Grads w/SmoothGrad','DeepLift', 'GradientSHAP', 'Feature Ablation', 'Weights']\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_title('Comparing input feature importances across multiple algorithms and learned weights')\n",
    "ax.set_ylabel('Attributions')\n",
    "\n",
    "ax.bar(x_axis_data, ig_attr_test_norm_sum, width, align='center', alpha=0.8, color='#eb5e7c')\n",
    "ax.bar(x_axis_data + width, ig_nt_attr_test_norm_sum, width, align='center', alpha=0.7, color='#A90000')\n",
    "ax.bar(x_axis_data + 2 * width, dl_attr_test_norm_sum, width, align='center', alpha=0.6, color='#34b8e0')\n",
    "ax.bar(x_axis_data + 3 * width, fa_attr_test_norm_sum, width, align='center', alpha=1.0, color='#49ba81')\n",
    "ax.autoscale_view()\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.set_xticks(x_axis_data + 0.5)\n",
    "ax.set_xticklabels(x_axis_data_labels)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(legends, loc=3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
